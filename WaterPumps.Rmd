---
title: "Water Pumps"
author: Steven Gusenius, Zuber Saiyed, Margarita Linets
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```
## About the Dataset
Using data from Taarifa and the Tanzanian Ministry of Water, we set out to predict what could impact the state of water pumps. 
A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.
More information about the challenge and the dataset can be found here - https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/

**MORE METADATA TO FOLLOW**
```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
#Libraries
library_list = list('ggplot2','glmnet','ggmap','reshape2', 'randomForest', 'gbm', 'caret','glmnetcr', 'knitr', 'xgboost')
lapply(library_list, require, character.only = TRUE)
```

## Import Data

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# Define train_values_url
train_values_url <- "http://s3.amazonaws.com/drivendata/data/7/public/4910797b-ee55-40a7-8668-10efd5c1b960.csv"
WaterPumps_value = read.csv(train_values_url, header=TRUE, stringsAsFactors = FALSE)
# Define train_labels_url
train_labels_url <- "http://s3.amazonaws.com/drivendata/data/7/public/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv"
WaterPumps_label = read.csv(train_labels_url, header=TRUE, stringsAsFactors = TRUE)
# Define test_values_url
test_values_url <- "http://s3.amazonaws.com/drivendata/data/7/public/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv"
WaterPumpsTest = read.csv(test_values_url, header=TRUE, stringsAsFactors = FALSE)
```

## Data Cleaning:

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
#Merge into a single datset
WaterPumps = merge(WaterPumps_value, WaterPumps_label, by='id')

#Convert all character variables to upper case and remove non alpha numeric variables 
charcols = which(sapply(WaterPumps,is.character))
WaterPumps[,charcols] <- sapply(WaterPumps[,charcols], toupper)
WaterPumps[,charcols] <- sapply(WaterPumps[, charcols], function(x) gsub("[^[:alnum:]=\\.]",'',x))

#Drop id variables - it is not a meanigful predict, just a record counter
WaterPumps = WaterPumps[, -which(names(WaterPumps) %in% c('id','recorded_by'))]
```


```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# Clean Data: Filter out variables with more than 30 factors - model matrix becomes too large
KeepVars = setNames(data.frame(sapply(WaterPumps[,which(sapply(WaterPumps, is.character))], function(x){if(length(unique(x))>30){FALSE} else {TRUE}})),
                    c('factorlevels'))
KeepVars$vars = rownames(KeepVars)
WaterPumps = WaterPumps[,-which(names(WaterPumps) %in% KeepVars$vars[which(KeepVars$factorlevels==FALSE)])]
```

## Exploratory Analysis: Visualizations  *ADD VISUALIZATIONS YOU LIKE TO THIS SECTION - WE WILL SELECT THE ONES WE LIKE AT THE END*
```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
qplot(quantity, data=WaterPumps, geom="bar", fill=status_group) +   theme(legend.position = "top")
qplot(quality_group, data=WaterPumps, geom="bar", fill=status_group) +  theme(legend.position = "top")
qplot(waterpoint_type, data=WaterPumps, geom="bar", fill=status_group) +
  theme(legend.position = "top") +
  theme(axis.text.x=element_text(angle = -20, hjust = 0))
qplot(region, data=WaterPumps, geom="bar", fill=status_group) +
  theme(legend.position = "top") +
  theme(axis.text.x=element_text(angle = -20, hjust = 0))
qplot(extraction_type_group, data=WaterPumps, geom="bar", fill=status_group) +
  theme(legend.position = "top") +
  theme(axis.text.x=element_text(angle = -20, hjust = 0))
qplot(source_type, data=WaterPumps, geom="bar", fill=status_group) +
  theme(legend.position = "top") +
  theme(axis.text.x=element_text(angle = -20, hjust = 0))
ggplot(subset(WaterPumps, construction_year > 0), aes(x = construction_year)) +
  geom_histogram(bins = 20) +
  facet_grid( ~ status_group)
```
```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
## Aggregate waterpump count by region
regional_geocodes = cbind.data.frame(data.frame("region" = unique(WaterPumps$region)),
                                setNames(data.frame(t(data.frame(lapply(lapply(unique(WaterPumps$region), function(x)paste0(x, ', Tanzania')), 
                                 function(x) as.numeric(geocode(x)))))), c('longitude','latitude')))

#Drop Row Names
rownames(regional_geocodes) <- c()
#Aggregate Pump Kinds by Region
PumpsByRegion=dcast(WaterPumps, region~status_group, fun=length, value.var = 'status_group')
PumpsByRegion = merge(PumpsByRegion, regional_geocodes, id='region')
colnames(PumpsByRegion)=c("region", "functional","needsrepair", "nonfunctional","longitude","latitude" )

Tanzania = as.numeric(geocode("Tanzania"))
Tanzania_Map = ggmap(get_googlemap(center=Tanzania, scale=2, zoom=6))+
  geom_point(aes(x=longitude, y=latitude), data=PumpsByRegion, col='blue', alpha=0.4,
             size=PumpsByRegion$functional*0.01)+
  geom_point(aes(x=longitude, y=latitude), data=PumpsByRegion, col='orange', alpha=0.4,
             size=PumpsByRegion$needsrepair*0.01)+
  geom_point(aes(x=longitude, y=latitude), data=PumpsByRegion, col='red', alpha=0.4,
             size=PumpsByRegion$nonfunctional*0.01)+
  ggtitle('Number of Pumps by Status by Region')
Tanzania_Map
```

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# Water Pump Position Plot

# Convert Latitude/Longitude to kilometers. 
#  - Use 1862 meters per nautical mile (nmi).
#  - Uses 60 nmi per degree of latitude
#  - Uses 60 nmi per degree of longitude (at the equator)
#  - Assumes spherical earth (for logitudes above and below the equator)

# Convert lat long to xy 
x = 1.862 * 60 * cos(WaterPumps$latitude*pi/180)*(WaterPumps$longitude)
y = 1.862 * 60 *(WaterPumps$latitude)

# Get Water Pump Operational State
z = as.factor(WaterPumps_label[,2])

# Plot Water pump Locations, color coded by OPerational state.  Use [x>10] indexing to elimninate bogus zeroed data entries.
plot(x[x>10],y[x>10],col=c("green","blue","red")[z[x>10]], xlab = 'Longitude (km)', ylab = 'Latitude (km)', pch = 18, main = 'Water Pump Locations (from Lon,Lat = [0,0])')
grid()
legend('bottomleft',legend = c('Functional','Repair','Non-Functional'), pch=18, col = c('green','blue','red'),inset = 0.01)
```


## Variable Selection

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# Take Only Non-Missing Values
WaterPumps <- WaterPumps[complete.cases(WaterPumps),]

# Create Model Matrix
modelmatrix = model.matrix(status_group~., WaterPumps)[,-1]

# Train and Validation Split
set.seed(500)
size=round(nrow(WaterPumps)*0.7)
train = sample(nrow(WaterPumps), size=size,replace = FALSE)
valid = which(!seq(1, nrow(WaterPumps), 1) %in% train)

train.x = modelmatrix[train,]
valid.x = modelmatrix[valid,]
train.y = WaterPumps$status_group[train]
valid.y = WaterPumps$status_group[valid]
```

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# LASSO
grid = 10^seq(10,-2, length=100)
set.seed(500)
lasso.mod = glmnet(train.x, train.y, alpha=1, lambda=grid, family = 'multinomial')

# Cross-Validate to Find Best Lambda
lasso.cv.out = cv.glmnet(train.x,train.y,alpha=1,lambda=grid, family = 'multinomial')
minlambda = lasso.cv.out$lambda.min
lasso.coefs = predict(lasso.mod,s=minlambda,type="coefficients")

lasso.coefs = cbind.data.frame(data.frame('Variable' = rownames(data.frame(lasso.coefs$functional[,1]))),
                               data.frame("Functional"=lasso.coefs$functional[,1]),
                               data.frame("NeedsRepair"=lasso.coefs$`functional needs repair`[,1]),
                               data.frame("NonFunctional"=lasso.coefs$`non functional`[,1]))
rownames(lasso.coefs) <- c()
lasso.coefs <- cbind(lasso.coefs, 'sum'= apply(lasso.coefs[,-which(names(lasso.coefs) %in% 'Variable')],1,sum))
varnames = as.character(lasso.coefs$Variable[lasso.coefs$sum!=0])
```

## Binary Outcome Lasso

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# Binary Outcome Lasso
train.df = cbind.data.frame(train.x, data.frame('status_group'=train.y))
for (each in levels(train.y)){
temp.y = train.y
levels(temp.y) = list(other=c(levels(train.y)[!levels(train.y) %in% each]),selected=each)
# Fit LASSO
fit.lasso = glmnet(train.x, temp.y, alpha=1, family = 'binomial')
# Cross-Validate to Find Best Lambda
lasso.cv.out = cv.glmnet(train.x,temp.y,alpha=1,family = 'binomial')
minlambda = lasso.cv.out$lambda.min
fitted = predict(fit.lasso, newx=train.x, s=minlambda, type='class')
print(confusionMatrix(fitted, temp.y))
}
```

## Multinomial Logit Model

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# Rita - Multinomial Logit
#--------------------------------------------------------------------------------------------------------------
modelmatrix_full = cbind.data.frame(modelmatrix, data.frame('status_group'=WaterPumps$status_group))

# Select equal sample sizes from all three categories:
functional.x = modelmatrix_full[modelmatrix_full$status_group=='functional',]
needsrepair.x = modelmatrix_full[modelmatrix_full$status_group=='functional needs repair',]
nonfunctional.x = modelmatrix_full[modelmatrix_full$status_group=='non functional',]

set.seed(500)
samplesize = sample(nrow(functional.x),4000,replace = FALSE)

functional.x.train = functional.x[samplesize,]
nonfunctional.x.train = nonfunctional.x[samplesize,]
needsrepair.x.train = needsrepair.x[samplesize[1:2000],]

functional.x.valid = functional.x[-samplesize,]
nonfunctional.x.valid = nonfunctional.x[-samplesize,]
needsrepair.x.valid = needsrepair.x[-samplesize[1:2000],]

dmtrain.x.balanced = rbind.data.frame(functional.x.train,nonfunctional.x.train,needsrepair.x.train)
dmvalid.x.balanced = rbind.data.frame(functional.x.valid,nonfunctional.x.valid,needsrepair.x.valid)

dmtrain.x.b = as.matrix(dmtrain.x.balanced[,-which(names(dmtrain.x.balanced) %in% 'status_group')])
dmtrain.y.b = as.factor(dmtrain.x.balanced$status_group)

dmvalid.x.b = as.matrix(dmvalid.x.balanced[,-which(names(dmvalid.x.balanced) %in% 'status_group')])
dmvalid.y.b = as.factor(dmvalid.x.balanced$status_group)

fit.glmnet.cr <- glmnet.cr(dmtrain.x.b, dmtrain.y.b, alpha=1)

#plot(fit.glmnet.cr, xvar = "step", type = "bic")
#plot(fit.glmnet.cr, xvar = "step", type = "coefficients")
BIC.step <- select.glmnet.cr(fit.glmnet.cr)
nonzero.glmnet.cr(fit.glmnet.cr, s = BIC.step)
lassomulti.fitted = fitted(fit.glmnet.cr, s=BIC.step)
lassomulti.predicted = predict(fit.glmnet.cr,newx=dmvalid.x.b)

minBIC = BIC.step[[1]]

confusionMatrix(factor(lassomulti.fitted$class, levels=levels(dmtrain.y.b)), dmtrain.y.b)
confusionMatrix(factor(lassomulti.predicted$class[,minBIC], levels=levels(dmvalid.y.b)), dmvalid.y.b)
```

BIC is minimized at step `r BIC.step[[1]]`


## Random Forest

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# Random Forest 
set.seed(500)
WaterPumps[sapply(WaterPumps, is.character)] <- lapply(WaterPumps[, which(sapply(WaterPumps, is.character))], as.factor)
RandomForest.mod = randomForest(status_group~., data=WaterPumps[train,], importance=TRUE, ntree=100)

#importance(RandomForest.mod)
varImpPlot(RandomForest.mod)
# Prediction error in the training data set
rf.fitted = predict(RandomForest.mod, WaterPumps[train,])
ConfTrainRF = confusionMatrix(rf.fitted, train.y)

# Prediction error in the validation dataset
rf.predicted = predict(RandomForest.mod, WaterPumps[valid,])
ConfValidRF = confusionMatrix(rf.predicted, valid.y)
```

From the random forest procedure, we obtained training set prediction accuracy of `r round(ConfTrainRF$overall["Accuracy"][[1]],3)*100`%. By contrast, in the validation set, the prediction accuracy was only `r round(ConfTrainRF$overall["Accuracy"][[1]],3)*100`%. 

## Random Forest with Boosting

```{r}
set.seed(500)
boost.mod=gbm(status_group~.,data=WaterPumps[train,],distribution="multinomial",n.trees=1000, interaction.depth=6, shrinkage=0.005)
summary(boost.mod)
# Prediction error in the training data set
boost.fitted = predict(boost.mod, newdata=WaterPumps[train,], n.trees = 500, type='response')
boost.fitted <- apply(boost.fitted, 1, which.max)
boost.fitted <- as.factor(ifelse(boost.fitted==1, 'functional',ifelse(boost.fitted==2, 'functional needs repair','non functional')))
ConfTrainBoost=confusionMatrix(boost.fitted, train.y)

# Prediction error in the test data set
boost.predicted = predict(boost.mod, newdata=WaterPumps[valid,], n.trees = 500, type='response')
boost.predicted <- apply(boost.predicted, 1, which.max)
boost.predicted <- as.factor(ifelse(boost.predicted==1, 'functional',ifelse(boost.predicted==2, 'functional needs repair','non functional')))
ConfValidBoost=confusionMatrix(boost.predicted, valid.y)
```

## XGBoost
XGBoost algorithm is one of the popular winning recipe of data science competitions. It has linear model solver as well as tree learning algorithm. We thought it would be a great way to learn and compare traditional algorithms with recently developed ones.
```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
library(dplyr)

features = names(WaterPumps) 
features = features[!features %in% c('status_group')]

xgboost.data = WaterPumps[,features]

response = as.numeric(WaterPumps$status_group) - 1
xgboost.modelmatrix = model.matrix(~., xgboost.data)
xgboost.train.x = xgboost.modelmatrix[train,2:ncol(xgboost.modelmatrix)]
xgboost.train.y = response[train]
xgboost.valid.x = xgboost.modelmatrix[valid,2:ncol(xgboost.modelmatrix)]
xgboost.valid.y = response[valid]

xgboost.test.data = WaterPumpsTest[, -which(names(WaterPumpsTest) %in% c('id','recorded_by'))]
charcols = which(sapply(xgboost.test.data,is.character))
xgboost.test.data[,charcols] <- sapply(xgboost.test.data[,charcols], toupper)
xgboost.test.matrix = model.matrix(~., xgboost.test.data)
xgboost.test.x = xgboost.test.matrix[,2:ncol(xgboost.test.matrix)]

no_of_classes = length(unique(response))

train_matrix = xgb.DMatrix(data  = xgboost.train.x, label = xgboost.train.y)

best_param = list()
best_logloss = Inf
best_logloss_index = 0

gen_params = function() {
      param <- list(objective = "multi:softmax",
          num_class = no_of_classes,
          max_depth = sample(7:15, 1),
          eta = runif(1, .03, .4),
          gamma = runif(1, 0.0, 0.3), 
          subsample = runif(1, .5, .9),
          colsample_bytree = runif(1, .5, .8), 
          min_child_weight = sample(1:20, 1),
          max_delta_step = sample(1:10, 1)
          )
     return(param)
}


for (iter in 1:10) {
    param = gen_params()
    str(param)
    cv.nround = 5
    cv.nfold = 5
    cv_model <- xgb.cv(data=train_matrix, params = param, nthread=6, 
                    nfold=cv.nfold, nrounds=cv.nround,
                    verbose = F,  metrics = list('merror'))
    min_logloss = min(cv_model$evaluation_log[, 'test_merror_mean'])
    min_logloss_index = which.min(unlist(cv_model$evaluation_log[, 'test_merror_mean']))
    if (min_logloss < best_logloss) {
        best_logloss = min_logloss
        best_logloss_index = min_logloss_index
        best_param = param
    }
}

str(best_param)
nround = 100
cv_best_model <- xgboost(data=train_matrix,params=best_param, nrounds=nround, nthread=6, verbose = F)

train_pred <- predict(cv_best_model, newdata = xgboost.train.x)
train_prediction = data.frame(prediction = train_pred + 1, label = xgboost.train.y + 1)
ConfTrainXGBoost = confusionMatrix(train_prediction$label, train_prediction$prediction)

valid_pred = predict(cv_best_model, newdata = xgboost.valid.x)
valid_prediction = data.frame(prediction = valid_pred + 1, label = xgboost.valid.y + 1)
ConfValidXGBoost = confusionMatrix(valid_prediction$label, valid_prediction$prediction)

#test_pred = predict(cv_best_model, newdata = xgboost.test.x)
#submit <- data.frame(WaterPumpsTest$id, test_pred)
#names(submit) <- c("id", "status_group")
#submit$status_group[submit$status_group==2] <- 'non functional'
#submit$status_group[submit$status_group==1] <- 'functional needs repair'
#submit$status_group[submit$status_group==0] <- 'functional'
#write.csv(submit, file = "pump_predictions.csv",row.names=FALSE)
#importance_matrix = xgb.importance(feature_names = features, model = cv_best_model)
#gp = xgb.plot.importance(importance_matrix)
```
From the random forest procedure, we obtained training set prediction accuracy of `r round(ConfTrainXGBoost$overall["Accuracy"][[1]],3)*100`%. 
By contrast, in the test set, the prediction accuracy was only `r round(ConfValidXGBoost$overall["Accuracy"][[1]],3)*100`%. 

```{r}

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
### Include model comparison figures here
```

## Conclusion

Based on the results above, random forest returns the best predictions.
