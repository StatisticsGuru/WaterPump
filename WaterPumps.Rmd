---
title: "Water Pumps"
author: Steven Gusenius, Zuber Saiyed, Margarita Linets
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE)
```
## About this Project
Using data from Taarifa and the Tanzanian Ministry of Water, we set out to predict what could impact the state of water pumps. 

A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.

More information about the challenge and the dataset can be found here - https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/

## About the Dataset
The datasets for this project were downloaded from www.drivendata.org and consisted of a two files of comma separated format.  This first file contains 40 characteristic data of each water pump, indexed by a pump ID, to be used as predictors. A list of these predictors is provided in APPENDIX A.

The second file contains the state_group for each water pump, also indexed by pump ID.  The state_group is the response we are attempting to predict and indicates the condition of a water pump.  Its value can be:  Functional (F), FunctionalNeedsRepair (FNR), or NonFunctional(NF).  

In total, there is data for 59,400 water pumps.

## Data Cleaning  
*Data Modification* Initially the datasets were cleaned to make them compatible with processing. Primarily this consisted of addressing missing data and special characters.  Then the predictor data was merged with the response data into a single dataset.

*Data Excluded* Following the merge, the pump ID was elimianted as it is not a meaningful predictor.  One predictor, **recorded_by** was excluded because it was constant for all water pumps. Several other cartegorical predictors were eliminated for having an excessive number of (greater than 30) levels.  A list of these factor variables, and their associated number of levels, is available in APPENDIX B. This step was needed when Lasso was used.  This because Lasso requires the inputs to be of type *model.matrix*.  A model matrix creates a separate column of data for each level of each factor variable.  This has a detrimental impact on both memory requirements and processing speed.  In this case, the retention of all such factor variables exceeded the capacity of the R software.  Further, it is a reasonable assumption that if a large proportion of the data is spread across many nomial factor levels, that factor variable will have diminished predictive power.   

For model types based on Random Forest, however, no such model matrix was required.  As such, for these approaches a separate dataset containing these variables was retained.


## Data Exploration
Prior to model fitting, some effort was invested in understanding the content of the data.  Various hypotheses were made and then evaluated through a number of simple, ad hoc analyses.

One such analysis was a data visualization where the frequency of the three **state_groups**, for each **region**, was plotted at the center of the respctive region on a map of Tanzania.  This provide an understanding of how the regions were dispersed throughout the country, an indication of how many water pumps were contained in each region, and quick assessment of whether each **region** had similar proportions of F, FNR, and NF water pumps.

From this map we observe that the distribution of the **regions** show no clear geometric pattern.  Further, likely due to differing sizes and populations, there is a lot of variability in the number of water pumps per **region**. Finally, it seems that if anything can be gleaned from the frequency data, it is that districts with the fewest water pumps might have a larger number that are NF.  However, this requires further study.

Additionally visual analysis was performed by plotting the position of each water pumps, color coded by **state_group**.  A simple spherical earth transformation allowed the water pumps longitude and latitude to be plotted in
on a flat plane with

Given its ability as a variable selector, Lasso fitting was employed as a means of assessing the which variables had the largest impact on the response. This ultimately allowed for a subset of candidate variables to be selected for future fittings. A list of the variables identified relevant is provided in APPENDIX C.

    -
    - Random Forest and Binomial Lasso perform variable selection

## Fit Approaches
    - training vs validation (size, purpose, etc)
    - Approach selection... lack of numerical data restricted what techniques could be used (nothing relying on euclidean distances)
    - relatively few FNR points made resulted, meant prediction performance of FNR was poor with some models types.
    - Approaches to deal with that: data balancing, one-vs-one binomial
    - description of each approach

    Binary Lasso Model (using One vs One)
    Random Forest
    Random Forest with Boosting
    XGBoost


**MORE METADATA TO FOLLOW**
```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
#Libraries
library_list = list('ggplot2','glmnet','ggmap','reshape2', 'randomForest', 'gbm', 'caret','glmnetcr', 'knitr', 'xgboost')
lapply(library_list, require, character.only = TRUE)
```

## Import Data

```{r, echo = TRUE, eval = TRUE}
# Define train_values_url
train_values_url <- "http://s3.amazonaws.com/drivendata/data/7/public/4910797b-ee55-40a7-8668-10efd5c1b960.csv"
WaterPumps_value = read.csv(train_values_url, header=TRUE, stringsAsFactors = FALSE)
# Define train_labels_url
train_labels_url <- "http://s3.amazonaws.com/drivendata/data/7/public/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv"
WaterPumps_label = read.csv(train_labels_url, header=TRUE, stringsAsFactors = TRUE)
# Define test_values_url
test_values_url <- "http://s3.amazonaws.com/drivendata/data/7/public/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv"
```

## Data Cleaning:

```{r, echo = TRUE, eval = TRUE}
#Merge into a single datset
WaterPumps = merge(WaterPumps_value, WaterPumps_label, by='id')

#Convert all character variables to upper case and remove non alpha numeric variables 
charcols = which(sapply(WaterPumps,is.character))
WaterPumps[,charcols] <- sapply(WaterPumps[,charcols], toupper)
WaterPumps[,charcols] <- sapply(WaterPumps[, charcols], function(x) gsub("[^[:alnum:]=\\.]",'',x))

#Drop id variables - it is not a meaningful predict, just a record counter
WaterPumps = WaterPumps[, -which(names(WaterPumps) %in% c('id','recorded_by'))]


# Convert Latitude/Longitude to kilometers.
#  - Uses 1862 meters per nautical mile (nmi).
#  - Uses 60 nmi per degree of latitude
#  - Uses 60 nmi per degree of longitude (at the equator)
#  - Assumes a spherical earth model (for logitudes above and below the equator)

# Convert lat long to xy
WaterPumps$East_km = 1.862 * 60 * cos(WaterPumps$latitude*pi/180)*(WaterPumps$longitude)
WaterPumps$North_km = 1.862 * 60 *(WaterPumps$latitude)

temp = rep(0, length(WaterPumps[,1]))
regionLevels = unique(WaterPumps$region_code)
for(i in 1 : length(regionLevels)) {
 ind = WaterPumps$region_code == regionLevels[i]
 temp[ind] = sum(ind)
}
WaterPumps$regionalPumpCount = temp


```


```{r}
# Clean Data: Filter out variables with more than 30 factors - model matrix becomes too large
KeepVars = setNames(data.frame(sapply(WaterPumps[,which(sapply(WaterPumps, is.character))], function(x){if(length(unique(x))>30){FALSE} else {TRUE}})),
                    c('factorlevels'))
KeepVars$vars = rownames(KeepVars)
WaterPumps = WaterPumps[,-which(names(WaterPumps) %in% KeepVars$vars[which(KeepVars$factorlevels==FALSE)])]

# Remove Multicollinearity

```

## Exploratory Analysis

### Population Vs Water Pump State
To check how population impacts the condition on water pumps, we aggregated population by region and grouped the status of water pumps per region.
As as population grows, no of non functional water pumps goes down. We can clearly see need of repair increase with population.
```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
populationSum = aggregate(population~region, sum, data=WaterPumps)
functionalByRegion =    aggregate(status_group~region, subset(WaterPumps,status_group=='functional'), function(x) length(x))
names(functionalByRegion) <- c("region", "functional")
nonfunctionalByRegion = aggregate(status_group~region, subset(WaterPumps,status_group=='non functional'), function(x) length(x))
names(nonfunctionalByRegion) <- c("region", "non functional")
repfunctionalByRegion = aggregate(status_group~region, subset(WaterPumps,status_group=='functional needs repair'), function(x) length(x))
names(repfunctionalByRegion) <- c("region", "functional needs repair")
regionpopstat = Reduce(function(x, y) merge(x, y,  by='region', all=TRUE), list(populationSum, functionalByRegion, nonfunctionalByRegion,repfunctionalByRegion))
names(regionpopstat) <- c('region','population', 'functional' ,'non_functional','needs_repair')
par(mfrow=c(1,3))
ggplot(regionpopstat, aes(population/1000, functional)) + geom_point() + stat_smooth()
ggplot(regionpopstat, aes(population/1000, non_functional)) + geom_point() + stat_smooth()
ggplot(regionpopstat, aes(population/1000, needs_repair)) + geom_point() + stat_smooth()
```
### Waterpoint Type Vs Water Pump State
Another important aspect to check is whether the issue is with the water delivery system or the source of water.  The following plot depicts various waterpoint types categorized by their working status. It’s clear from the plot that ‘communal standpipe’ and ‘improved spring’ water point types require more maintenance than ‘hand pump’ or ‘cattle through’.
```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
watertype = data.frame(prop.table(table(WaterPumps$waterpoint_type_group, WaterPumps$status_group), margin = 1))
ggplot(watertype, aes(Var1, Freq, fill = Var2)) +
    geom_bar(position = "dodge", stat = "identity") +
    scale_y_continuous(labels=scales::percent) +
   labs(x="Waterpoint Type",y="%") +
    theme(axis.text.x=element_text(angle = -20, hjust = 0))
```
### Management Vs Water Pump State
Who manages the system is also a good indicator to explore. It seems like management by a commercial or political entity does result in more functional water pumps. Clearly, water pumps managed by user groups are not doing so great!
```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
mgmttype = data.frame(prop.table(table(WaterPumps$management_group, WaterPumps$status_group), margin = 1))
ggplot(mgmttype, aes(Var1, Freq, fill = Var2)) +
    geom_bar(position = "dodge", stat = "identity") +
    scale_y_continuous(labels=scales::percent) +
   labs(x="Mgmt Type",y="%") +
    theme(axis.text.x=element_text(angle = -20, hjust = 0))
```

It's also important to mention how payment scheme helps in better condition of water pumps.
```{r}
paymenttype = data.frame(prop.table(table(WaterPumps$payment_type, WaterPumps$status_group), margin = 1))
ggplot(paymenttype, aes(Var1, Freq, fill = Var2)) +
    geom_bar(position = "dodge", stat = "identity") +
    scale_y_continuous(labels=scales::percent) +
   labs(x="Mgmt Type",y="%") +
    theme(axis.text.x=element_text(angle = -20, hjust = 0))
```
Here is further analysis of the water pumps which are being used for free.
It's not surprise that commericially operated water pumps are more non functional if they are used for free.
Same is the case for any other management scheme unless goverment or any political entity operates, in that case more water pumps are functional.
```{r}
neverpaydf = WaterPumps[WaterPumps$payment_type == 'NEVERPAY',]
neverpaytype = data.frame(prop.table(table(neverpaydf$management_group, neverpaydf$status_group), margin = 1))
ggplot(neverpaytype, aes(Var1, Freq, fill = Var2)) +
    geom_bar(position = "dodge", stat = "identity") +
    scale_y_continuous(labels=scales::percent) +
   labs(x="Mgmt Type",y="%") +
    theme(axis.text.x=element_text(angle = -20, hjust = 0))
```

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }

qplot(waterpoint_type_group, data=WaterPumps, geom="bar", fill=status_group) +
  theme(legend.position = "top") +
  theme(axis.text.x=element_text(angle = -20, hjust = 0))
qplot(quantity, data=WaterPumps, geom="bar", fill=status_group) +   theme(legend.position = "top")
qplot(quality_group, data=WaterPumps, geom="bar", fill=status_group) +  theme(legend.position = "top")
qplot(waterpoint_type_group, data=watertype, geom="bar", fill=status_group) +
  theme(legend.position = "top") +
  theme(axis.text.x=element_text(angle = -20, hjust = 0))
qplot(region, data=WaterPumps, geom="bar", fill=status_group) +
  theme(legend.position = "top") +
  theme(axis.text.x=element_text(angle = -20, hjust = 0))
qplot(extraction_type_group, data=WaterPumps, geom="bar", fill=status_group) +
  theme(legend.position = "top") +
  theme(axis.text.x=element_text(angle = -20, hjust = 0))
qplot((WaterPumps$population), data=WaterPumps, geom="bar", fill=status_group) +
  theme(legend.position = "top") +
  theme(axis.text.x=element_text(angle = -20, hjust = 0))
ggplot(subset(WaterPumps, construction_year > 0), aes(x = construction_year)) +
  geom_histogram(bins = 20) +
  facet_grid( ~ status_group)
```

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
## Aggregate waterpump count by region
regional_geocodes = cbind.data.frame(data.frame("region" = unique(WaterPumps$region)),
                                     setNames(data.frame(t(data.frame(lapply(lapply(unique(WaterPumps$region), function(x)paste0(x, ', Tanzania')), 
                                                                             function(x) as.numeric(geocode(x)))))), c('longitude','latitude')))

#Drop Row Names
rownames(regional_geocodes) <- c()
#Aggregate Pump Kinds by Region
PumpsByRegion=dcast(WaterPumps, region~status_group, fun=length, value.var = 'status_group')
PumpsByRegion = merge(PumpsByRegion, regional_geocodes, id='region')
colnames(PumpsByRegion)=c("region", "functional","needsrepair", "nonfunctional","longitude","latitude" )

Tanzania = as.numeric(geocode("Tanzania"))
Tanzania_Map = ggmap(get_googlemap(center=Tanzania, scale=2, zoom=6))+
    geom_point(aes(x=longitude, y=latitude), data=PumpsByRegion, col='blue', alpha=0.4,
               size=PumpsByRegion$functional*0.01)+
    geom_point(aes(x=longitude, y=latitude), data=PumpsByRegion, col='orange', alpha=0.4,
               size=PumpsByRegion$needsrepair*0.01)+
    geom_point(aes(x=longitude, y=latitude), data=PumpsByRegion, col='red', alpha=0.4,
               size=PumpsByRegion$nonfunctional*0.01)+
    ggtitle('Number of Pumps by Status by Region')
Tanzania_Map
```

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# Water Pump Position Plot

# Get Water Pump Operational State
z = as.factor(WaterPumps_label[,2])

# Plot Water pump Locations, color coded by Operational state.  Use [x>10] indexing to elimninate bogus zeroed data entries.
inds = WaterPumps$East_km > 20
plot(WaterPumps$East_km[inds],WaterPumps$North_km[inds],col=c("green","blue","red")[z[inds]], xlab = 'Longitude (km)', ylab = 'Latitude (km)', pch = 18, main = 'Water Pump Locations (from Lon,Lat = [0,0])')
grid()
legend('bottomleft',legend = c('Functional','Repair','Non-Functional'), pch=18, col = c('green','blue','red'),inset = 0.01)
```

## Train & Validation Split

```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
# Take Only Non-Missing Values
WaterPumps <- WaterPumps[complete.cases(WaterPumps),]

# Create Model Matrix
modelmatrix = model.matrix(status_group~., WaterPumps)[,-1]

# Train and Validation Split
set.seed(500)
size=round(nrow(WaterPumps)*0.7)
train = sample(nrow(WaterPumps), size=size,replace = FALSE)
valid = which(!seq(1, nrow(WaterPumps), 1) %in% train)

train.x = modelmatrix[train,]
valid.x = modelmatrix[valid,]
train.y = WaterPumps$status_group[train]
valid.y = WaterPumps$status_group[valid]
```

## Results

# Binary Outcome Lasso

```{r}
# Binary Outcome Lasso
temp = rep(0,length(valid.y))
Predict.Valid = data.frame(model1 = temp,
                       model2 = temp,
                       model3 = temp)

temp = rep(0,length(train.y))
Predict.Train = data.frame(model1 = temp,
                       model2 = temp,
                       model3 = temp)

LevelName = ''
train.df = cbind.data.frame(train.x, data.frame('status_group'=train.y))
i = 0

# Build a model for Each Level and record its predictions
for (each in levels(train.y)){

    i = i + 1
    LevelName[i] = each
    temp.y = train.y
    levels(temp.y) = list(other=c(levels(train.y)[!levels(train.y) %in% each]),selected=each)
    
    # Fit LASSO
    fit.lasso = glmnet(train.x, temp.y, alpha=1, family = 'binomial')
    
    # Cross-Validate to Find Best Lambda
    lasso.cv.out = cv.glmnet(train.x,temp.y,alpha=1,family = 'binomial')
    minlambda = lasso.cv.out$lambda.min
    
    # fitted = predict(fit.lasso, newx=train.x, s=minlambda, type='class')
    Predict.Valid[,i] = predict(fit.lasso, newx=valid.x, s=minlambda, type = 'response')
    Predict.Train[,i] = predict(fit.lasso, newx=train.x, s=minlambda, type = 'response')
}

bestCatValid = rep('other',length(valid.y))

for (k in 1 : length(valid.y)) {
    bestCatValid[k] = LevelName[which.max(Predict.Valid[k,])]
}

bestCatTrain = rep('other',length(train.y))

for (k in 1 : length(train.y)) {
    bestCatTrain[k] = LevelName[which.max(Predict.Train[k,])]
}

Binary.ConfTrain = confusionMatrix(bestCatTrain, train.y)
Binary.ConfValid = confusionMatrix(bestCatValid, valid.y)
```


```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
kable(data.frame(Binary.ConfTrain$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')],
      caption = 'Training Data Performance')

kable(data.frame(Binary.ConfValid$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')],
      caption = 'Validation Data Performance')
```

# Random Forest

```{r}
# Random Forest 
set.seed(500)
WaterPumps[sapply(WaterPumps, is.character)] <- lapply(WaterPumps[, which(sapply(WaterPumps, is.character))], as.factor)
RandomForest.mod = randomForest(status_group~., data=WaterPumps[train,], importance=TRUE, ntree=100)
#importance(RandomForest.mod)
varImpPlot(RandomForest.mod)
# Prediction error in the training data set
rf.fitted = predict(RandomForest.mod, WaterPumps[train,])
RF.ConfTrain = confusionMatrix(rf.fitted, train.y)
# Prediction error in the validation dataset
rf.predicted = predict(RandomForest.mod, WaterPumps[valid,])
RF.ConfValid = confusionMatrix(rf.predicted, valid.y)
```

From the random forest procedure, we obtained training set prediction accuracy of `r round(ConfTrain$overall["Accuracy"][[1]],3)*100`%. By contrast, in the validation set, the prediction accuracy was only `r round(ConfValid$overall["Accuracy"][[1]],3)*100`%. 


```{r}
kable(data.frame(RF.ConfTrain$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')],
      caption = 'Training Data Performance')

kable(data.frame(RF.ConfValid$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')],
      caption = 'Validation Data Performance')
```

# Random Forest with Boosting

```{r}
set.seed(500)
boost.mod=gbm(status_group~.,data=WaterPumps[train,],distribution="multinomial",n.trees=1000, interaction.depth=6, shrinkage=0.005)
summary(boost.mod)
# Prediction error in the training data set
boost.fitted = predict(boost.mod, newdata=WaterPumps[train,], n.trees = 500, type='response')
boost.fitted <- apply(boost.fitted, 1, which.max)
boost.fitted <- as.factor(ifelse(boost.fitted==1, 'functional',ifelse(boost.fitted==2, 'functional needs repair','non functional')))
Boost.ConfTrain=confusionMatrix(boost.fitted, train.y)

# Prediction error in the test data set
boost.predicted = predict(boost.mod, newdata=WaterPumps[valid,], n.trees = 500, type='response')
boost.predicted <- apply(boost.predicted, 1, which.max)
boost.predicted <- as.factor(ifelse(boost.predicted==1, 'functional',ifelse(boost.predicted==2, 'functional needs repair','non functional')))
Boost.ConfValid=confusionMatrix(boost.predicted, valid.y)
```

```{r}
kable(data.frame(Boost.ConfTrain$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')],
      caption = 'Training Data Performance')

From the random forest procedure, we obtained training set prediction accuracy of `r round(ConfTrainXGBoost$overall["Accuracy"][[1]],3)*100`%.
By contrast, in the test set, the prediction accuracy was only `r round(ConfValidXGBoost$overall["Accuracy"][[1]],3)*100`%.

## Evaluation & Selection

kable(data.frame(Boost.ConfValid$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')],
      caption = 'Validation Data Performance')
```
## XGBoost
XGBoost algorithm is one of the popular winning recipe of data science competitions. It has linear model solver as well as tree learning algorithm. We thought it would be a great way to learn and compare traditional algorithms with recently developed ones.
```{r echo = TRUE, include = TRUE, messages = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=90) }
library(dplyr)

features = names(WaterPumps)
features = features[!features %in% c('status_group')]

xgboost.data = WaterPumps[,features]

response = as.numeric(WaterPumps$status_group) - 1
xgboost.modelmatrix = model.matrix(~., xgboost.data)
xgboost.train.x = xgboost.modelmatrix[train,2:ncol(xgboost.modelmatrix)]
xgboost.train.y = response[train]
xgboost.valid.x = xgboost.modelmatrix[valid,2:ncol(xgboost.modelmatrix)]
xgboost.valid.y = response[valid]

xgboost.test.data = WaterPumpsTest[, -which(names(WaterPumpsTest) %in% c('id','recorded_by'))]
charcols = which(sapply(xgboost.test.data,is.character))
xgboost.test.data[,charcols] <- sapply(xgboost.test.data[,charcols], toupper)
xgboost.test.matrix = model.matrix(~., xgboost.test.data)
xgboost.test.x = xgboost.test.matrix[,2:ncol(xgboost.test.matrix)]

no_of_classes = length(unique(response))

train_matrix = xgb.DMatrix(data  = xgboost.train.x, label = xgboost.train.y)

best_param = list()
best_logloss = Inf
best_logloss_index = 0

gen_params = function() {
      param <- list(objective = "multi:softmax",
          num_class = no_of_classes,
          max_depth = sample(7:15, 1),
          eta = runif(1, .03, .4),
          gamma = runif(1, 0.0, 0.3),
          subsample = runif(1, .5, .9),
          colsample_bytree = runif(1, .5, .8),
          min_child_weight = sample(1:20, 1),
          max_delta_step = sample(1:10, 1)
          )
     return(param)
}


for (iter in 1:10) {
    param = gen_params()
    str(param)
    cv.nround = 5
    cv.nfold = 5
    cv_model <- xgb.cv(data=train_matrix, params = param, nthread=6,
                    nfold=cv.nfold, nrounds=cv.nround,
                    verbose = F,  metrics = list('merror'))
    min_logloss = min(cv_model$evaluation_log[, 'test_merror_mean'])
    min_logloss_index = which.min(unlist(cv_model$evaluation_log[, 'test_merror_mean']))
    if (min_logloss < best_logloss) {
        best_logloss = min_logloss
        best_logloss_index = min_logloss_index
        best_param = param
    }
}

str(best_param)
nround = 100
cv_best_model <- xgboost(data=train_matrix,params=best_param, nrounds=nround, nthread=6, verbose = F)

train_pred <- predict(cv_best_model, newdata = xgboost.train.x)
train_prediction = data.frame(prediction = train_pred + 1, label = xgboost.train.y + 1)
ConfTrainXGBoost = confusionMatrix(train_prediction$label, train_prediction$prediction)

valid_pred = predict(cv_best_model, newdata = xgboost.valid.x)
valid_prediction = data.frame(prediction = valid_pred + 1, label = xgboost.valid.y + 1)
ConfValidXGBoost = confusionMatrix(valid_prediction$label, valid_prediction$prediction)

#test_pred = predict(cv_best_model, newdata = xgboost.test.x)
#submit <- data.frame(WaterPumpsTest$id, test_pred)
#names(submit) <- c("id", "status_group")
#submit$status_group[submit$status_group==2] <- 'non functional'
#submit$status_group[submit$status_group==1] <- 'functional needs repair'
#submit$status_group[submit$status_group==0] <- 'functional'
#write.csv(submit, file = "pump_predictions.csv",row.names=FALSE)
#importance_matrix = xgb.importance(feature_names = features, model = cv_best_model)
#gp = xgb.plot.importance(importance_matrix)
```


## Conclusions

```{r}
Accuracy = data.frame('Model' = c('Binary Outcome Lasso','Random Forest','Boosted Random Forest'),
                      'Accuracy' = c(Binary.ConfValid$overall[['Accuracy']][[1]],RF.ConfValid$overall[['Accuracy']][[1]],
                                     Boost.ConfValid$overall[['Accuracy']][[1]]))
kable(Accuracy, caption = 'Model Comparison')
```


## Appendices
    - Spatial plots
    - Source code