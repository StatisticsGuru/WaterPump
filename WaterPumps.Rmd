---
title: "Water Pumps"
author: Steven Gusenius, Zuber Saiyed, Margarita Linets
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE)
```
## About this Project
Using data from Taarifa and the Tanzanian Ministry of Water, we set out to predict where water pumps were likely to be functional, in need of repair of not functional at a certainl locale. 

A smart understanding of which water pumps will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.

More information about the challenge and the dataset can be found here - https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/

## About the Dataset
The datasets for this project were downloaded from www.drivendata.org and consisted of a two files of comma separated format.  This first file contains 40 characteristic data of each water pump, indexed by a pump ID, to be used as predictors. A list of these predictors is provided in *APPENDIX A*.

The second file contains the status_group for each water pump, also indexed by pump ID.  The status_group is the response we are attempting to predict and indicates the condition of a water pump.  Its value can be:  Functional (F), FunctionalNeedsRepair (FNR), or NonFunctional(NF).  The respective percentages of each are: 54.3%, 7.3%, 38.4%.

In total, there is data for 59,400 water pumps.

## Data Cleaning  
*Data Modification* Initially the datasets were cleaned to make them compatible with processing. Primarily this consisted of addressing missing data and special characters.  Then the predictor data was merged with the response data into a single dataset.

*Data Excluded* Following the merge, the pump ID was eliminated as it is not a meaningful predictor.  One predictor, **recorded_by** was excluded because it had minimal variation for all water pumps. Several other categorical predictors were eliminated for having an excessive number of (greater than 30) levels.  A list of these factor variables, and their associated number of levels, is available in *APPENDIX B*. This step was needed when Lasso was used.  This because Lasso requires the inputs to be of type *model.matrix*.  A model matrix creates a separate column of data for each level of each factor variable.  This has a detrimental impact on both memory requirements and processing speed.  In this case, the retention of all such factor variables exceeded the capacity of the R software.  Further, it is a reasonable assumption that if a large proportion of the data is spread across many nominal factor levels, that factor variable will have diminished predictive power.   


## Data Exploration
Prior to model fitting, some effort was invested in understanding the content of the data.  Various hypotheses were made and then evaluated through a number of simple, ad hoc analyses.

One such analysis was a data visualization where the frequency of the three **status_groups**, for each **region**, was plotted at the center of the respective region on a map of Tanzania.  This map provide an understanding of how pump functionality was dispersed throughout the country, and serves as an indication of how many water pumps were contained in each region and whether each **region** had similar proportions of F, FNR, and NF water pumps.

Based on this map, it appears that districts with the fewest water pumps might have a larger number of pumps that are NF.  For this reason a variable **regionalPumpCount** was added to the dataset. 

```{r}
#Libraries
library_list = list('ggplot2','glmnet','ggmap','reshape2', 'randomForest', 'gbm', 'caret','knitr')
lapply(library_list, require, character.only = TRUE)
```

```{r}
# Import Data
# Define train_values_url
train_values_url <- "http://s3.amazonaws.com/drivendata/data/7/public/4910797b-ee55-40a7-8668-10efd5c1b960.csv"
WaterPumps_value = read.csv(train_values_url, header=TRUE, stringsAsFactors = FALSE)
# Define train_labels_url
train_labels_url <- "http://s3.amazonaws.com/drivendata/data/7/public/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv"
WaterPumps_label = read.csv(train_labels_url, header=TRUE, stringsAsFactors = TRUE)
# Define test_values_url
test_values_url <- "http://s3.amazonaws.com/drivendata/data/7/public/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv"
test_values = read.csv(test_values_url, header=TRUE, stringsAsFactors=TRUE)
```

```{r,fig.width=7, fig.height=6}
### Data Cleaning
#Merge into a single datset
WaterPumps = merge(WaterPumps_value, WaterPumps_label, by='id')

#Convert all character variables to upper case and remove non alpha numeric variables 
charcols = which(sapply(WaterPumps,is.character))
WaterPumps[,charcols] <- sapply(WaterPumps[,charcols], toupper)
WaterPumps[,charcols] <- sapply(WaterPumps[, charcols], function(x) gsub("[^[:alnum:]=\\.]",'',x))

#Drop id variables - it is not a meaningful predict, just a record counter
WaterPumps = WaterPumps[, -which(names(WaterPumps) %in% c('id','recorded_by'))]


# Convert Latitude/Longitude to kilometers. 
#  - Uses 1862 meters per nautical mile (nmi).
#  - Uses 60 nmi per degree of latitude
#  - Uses 60 nmi per degree of longitude (at the equator)
#  - Assumes a spherical earth model (for logitudes above and below the equator)

# Convert lat long to xy 
WaterPumps$East_km = 1.862 * 60 * cos(WaterPumps$latitude*pi/180)*(WaterPumps$longitude)
WaterPumps$North_km = 1.862 * 60 *(WaterPumps$latitude)
WaterPumps = WaterPumps[ ,-which(names(WaterPumps) %in% c('longitude','latitude')) ]

temp = rep(0, length(WaterPumps[,1]))
regionLevels = unique(WaterPumps$region_code)
for(i in 1 : length(regionLevels)) {
 ind = WaterPumps$region_code == regionLevels[i]
 temp[ind] = sum(ind)
}
WaterPumps$regionalPumpCount = temp


```


```{r, echo = TRUE, eval = TRUE}
# Clean Data: Filter out variables with more than 30 factors - model matrix becomes too large
KeepVars = setNames(data.frame(sapply(WaterPumps[,which(sapply(WaterPumps, is.character))], function(x){if(length(unique(x))>30){FALSE} else {TRUE}})),
                    c('factorlevels'))
KeepVars$vars = rownames(KeepVars)
WaterPumps = WaterPumps[,-which(names(WaterPumps) %in% KeepVars$vars[which(KeepVars$factorlevels==FALSE)])]

# Remove Multicollinearity

```

```{r, fig.width=7, fig.height=6}
###Exploratory Analysis
## Aggregate waterpump count by region
regional_geocodes = cbind.data.frame(data.frame("region" = unique(WaterPumps$region)),
                                     setNames(data.frame(t(data.frame(lapply(lapply(unique(WaterPumps$region), function(x)paste0(x, ', Tanzania')), 
                                                                             function(x) as.numeric(geocode(x)))))), c('longitude','latitude')))

#Drop Row Names
rownames(regional_geocodes) <- c()
#Aggregate Pump Kinds by Region
PumpsByRegion=dcast(WaterPumps, region~status_group, fun=length, value.var = 'status_group')
PumpsByRegion = merge(PumpsByRegion, regional_geocodes, id='region')
colnames(PumpsByRegion)=c("region", "functional","needsrepair", "nonfunctional","longitude","latitude" )

Tanzania = as.numeric(geocode("Tanzania"))
Tanzania_Map = ggmap(get_googlemap(center=Tanzania, scale=2, zoom=6))+
    geom_point(aes(x=longitude, y=latitude), data=PumpsByRegion, col='blue', alpha=0.4,
               size=PumpsByRegion$functional*0.01)+
    geom_point(aes(x=longitude, y=latitude), data=PumpsByRegion, col='orange', alpha=0.4,
               size=PumpsByRegion$needsrepair*0.01)+
    geom_point(aes(x=longitude, y=latitude), data=PumpsByRegion, col='red', alpha=0.4,
               size=PumpsByRegion$nonfunctional*0.01)+
    ggtitle('Number of Pumps by Status by Region')
Tanzania_Map
```

Additionally visual analysis was performed by plotting the position of each water pumps, color coded by **status_group**.  A simple spherical earth transformation allowed the water pump positions in longitude and latitude to be plotted in on a flat plane using linear units of kilometers. Given that linear units are preferable for fitting models, these transformed positions were added to the dataset as **East_km** and **North_km**.  Because these would exhibit exceptionally high correlation with **longitude** and **latitude**, respectively, the latter variables were removed from the dataset.


```{r}
# Water Pump Position Plot

# Get Water Pump Operational State
z = as.factor(WaterPumps_label[,2])

# Plot Water pump Locations, color coded by Operational state.  Used [East_km > 20] indexing to elimninate bogus zeroed data entries from the plot.
inds = WaterPumps$East_km > 20
plot(WaterPumps$East_km[inds],WaterPumps$North_km[inds],col=c("green","blue","red")[z[inds]], xlab = 'Longitude (km)', ylab = 'Latitude (km)', pch = 18, main = 'Water Pump Locations (from Lon,Lat = [0,0])')
grid()
legend('bottomleft',legend = c('Functional','Repair','Non-Functional'), pch=18, col = c('green','blue','red'),inset = 0.01)
```

The plot above was examined to see if there were signs of clustering among pumps of a specific **status_group**.  While it did appear that there were some areas of the country with elevated proportions NF pumps, there was no recognizable pattern that could be leveraged for this evaluation.  A visual comparison against a mean annual rainfall map of Tanzania (available on the internet), looked like it might exhibit correlation between areas with more rain and the location of all water pumps.  A similar map of Tanzania average temperatures showed a potential positive correlate between hot temperatures and NF pumps.  However, defining these relationships is beyond the scope of this effort.


## Fit Approaches

For all models, cross validation was used.  This consisted of separating the data into **training** and **validation** sets.  The models were constructed using the **training** data, then their performances were evaluated using the **validation** data.  The split between the two sets was approximately 70% **training** and 30% **validation**.
    
Given that relatively few of the variables contained numeric data, model approaches that utilize Euclidean distances between datapoints could not be used. 

Because a small proportion of water pumps were of **status_group** FNR, some model types would ignore this state completely their predictions.

One approach for addressing this was the use of a Binary Outcome Lasso using a one-vs-one selection strategy.  With this strategy, three **sub-models** were built.  Each sub-model was assigned a level of the response variable.  The remaining two levels were given the value of **other**.  This forced the sub-model to focus on fitting only its assigned level.  The outcome of each sub-model was an estimated probability that the each datapoint belonged to the assigned level.  Each point was assessed against these three sets of predictions.  The level with the highest probability was selected. 

Given their general suitability for datasets of this nature, Random Forest and Random Forest with Boosting were also used.

```{r}
# Train and Validation Split
# Take Only Non-Missing Values
WaterPumps <- WaterPumps[complete.cases(WaterPumps),]

# Create Model Matrix
modelmatrix = model.matrix(status_group~., WaterPumps)[,-1]

# Train and Validation Split
set.seed(500)
size=round(nrow(WaterPumps)*0.7)
train = sample(nrow(WaterPumps), size=size,replace = FALSE)
valid = which(!seq(1, nrow(WaterPumps), 1) %in% train)

train.x = modelmatrix[train,]
valid.x = modelmatrix[valid,]
train.y = WaterPumps$status_group[train]
valid.y = WaterPumps$status_group[valid]
```

# Results

## Binary Outcome Lasso

```{r}
# Binary Outcome Lasso 
temp = rep(0,length(valid.y))
Predict.Valid = data.frame(model1 = temp,
                       model2 = temp,
                       model3 = temp)

temp = rep(0,length(train.y))
Predict.Train = data.frame(model1 = temp,
                       model2 = temp,
                       model3 = temp)

LevelName = ''
train.df = cbind.data.frame(train.x, data.frame('status_group'=train.y))
i = 0

# Build a model for Each Level and record its predictions
for (each in levels(train.y)){

    i = i + 1
    LevelName[i] = each
    temp.y = train.y
    levels(temp.y) = list(other=c(levels(train.y)[!levels(train.y) %in% each]),selected=each)
    
    # Fit LASSO
    fit.lasso = glmnet(train.x, temp.y, alpha=1, family = 'binomial')
    
    # Cross-Validate to Find Best Lambda
    lasso.cv.out = cv.glmnet(train.x,temp.y,alpha=1,family = 'binomial')
    minlambda = lasso.cv.out$lambda.min
    
    # fitted = predict(fit.lasso, newx=train.x, s=minlambda, type='class')
    Predict.Valid[,i] = predict(fit.lasso, newx=valid.x, s=minlambda, type = 'response')
    Predict.Train[,i] = predict(fit.lasso, newx=train.x, s=minlambda, type = 'response')
}

bestCatValid = rep('other',length(valid.y))

for (k in 1 : length(valid.y)) {
    bestCatValid[k] = LevelName[which.max(Predict.Valid[k,])]
}


bestCatTrain = rep('other',length(train.y))

for (k in 1 : length(train.y)) {
    bestCatTrain[k] = LevelName[which.max(Predict.Train[k,])]
}

Binary.ConfTrain = confusionMatrix(bestCatTrain, train.y)
Binary.ConfValid = confusionMatrix(bestCatValid, valid.y)
```


```{r}
kable(data.frame(Binary.ConfTrain$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')], 
      caption = 'Training Data Performance')

kable(data.frame(Binary.ConfValid$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')], 
      caption = 'Validation Data Performance')
```

## Random Forest 

```{r}
# Random Forest 
set.seed(500)
WaterPumps[sapply(WaterPumps, is.character)] <- lapply(WaterPumps[, which(sapply(WaterPumps, is.character))], as.factor)
RandomForest.mod = randomForest(status_group~., data=WaterPumps[train,], importance=TRUE, ntree=100)
#importance(RandomForest.mod)
varImpPlot(RandomForest.mod)
# Prediction error in the training data set
rf.fitted = predict(RandomForest.mod, WaterPumps[train,])
RF.ConfTrain = confusionMatrix(rf.fitted, train.y)
# Prediction error in the validation dataset
rf.predicted = predict(RandomForest.mod, WaterPumps[valid,])
RF.ConfValid = confusionMatrix(rf.predicted, valid.y)
```

From the random forest procedure, we obtained training set prediction accuracy of `r round(ConfTrain$overall["Accuracy"][[1]],3)*100`%. By contrast, in the validation set, the prediction accuracy was only `r round(ConfValid$overall["Accuracy"][[1]],3)*100`%. 


```{r}
kable(data.frame(RF.ConfTrain$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')], 
      caption = 'Training Data Performance')

kable(data.frame(RF.ConfValid$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')], 
      caption = 'Validation Data Performance')
```

## Random Forest with Boosting 

```{r}
set.seed(500)
boost.mod=gbm(status_group~.,data=WaterPumps[train,],distribution="multinomial",n.trees=1000, interaction.depth=6, shrinkage=0.005)
summary(boost.mod)
# Prediction error in the training data set
boost.fitted = predict(boost.mod, newdata=WaterPumps[train,], n.trees = 500, type='response')
boost.fitted <- apply(boost.fitted, 1, which.max)
boost.fitted <- as.factor(ifelse(boost.fitted==1, 'functional',ifelse(boost.fitted==2, 'functional needs repair','non functional')))
Boost.ConfTrain=confusionMatrix(boost.fitted, train.y)

# Prediction error in the test data set
boost.predicted = predict(boost.mod, newdata=WaterPumps[valid,], n.trees = 500, type='response')
boost.predicted <- apply(boost.predicted, 1, which.max)
boost.predicted <- as.factor(ifelse(boost.predicted==1, 'functional',ifelse(boost.predicted==2, 'functional needs repair','non functional')))
Boost.ConfValid=confusionMatrix(boost.predicted, valid.y)
```

```{r}
kable(data.frame(Boost.ConfTrain$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')], 
      caption = 'Training Data Performance')

kable(data.frame(Boost.ConfValid$byClass)[,c('Sensitivity', 'Specificity', 'Precision','Recall','Balanced.Accuracy')], 
      caption = 'Validation Data Performance')
```

# Conclusion

Based on the results obtained with the previous models, it is evident that the random forest peroforms best. Below is the comparison of prediction accuracy of all three models over the validation dataset. 

```{r}
Accuracy = data.frame('Model' = c('Binary Outcome Lasso','Random Forest','Boosted Random Forest'),
                      'Accuracy' = c(Binary.ConfValid$overall[['Accuracy']][[1]],RF.ConfValid$overall[['Accuracy']][[1]],
                                     Boost.ConfValid$overall[['Accuracy']][[1]]))
kable(Accuracy, caption = 'Model Comparison')
```
    

# Appendices

## Appendix A

```{r, echo=FALSE}
metadata = data.frame('Variable'= c('amount_tsh', 'date_recorded', 'funder', 'gps_height', 'installer', 'longitude', 'latitude', 'wpt_name', 'num_private', 'basin', 'subvillage', 'region', 'region_code', 'district_code', 'lga', 'ward', 'population', 'public_meeting', 'recorded_by', 'scheme_management', 'scheme_name', 'permit', 'construction_year', 'extraction_type', 'extraction_type_group', 'extraction_type_class', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'quantity_group', 'source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group'),
                      'Definition' = c(' Total static head (amount water available to waterpoint)', ' The date the row was entered', ' Who funded the well', ' Altitude of the well', ' Organization that installed the well', ' GPS coordinate', ' GPS coordinate', ' Name of the waterpoint if there is one', 'Num Private', ' Geographic water basin', ' Geographic location', ' Geographic location', ' Geographic location (coded)', ' Geographic location (coded)', ' Geographic location', ' Geographic location', ' Population around the well', ' True/False', ' Group entering this row of data', ' Who operates the waterpoint', ' Who operates the waterpoint', ' If the waterpoint is permitted', ' Year the waterpoint was constructed', ' The kind of extraction the waterpoint uses', ' The kind of extraction the waterpoint uses', ' The kind of extraction the waterpoint uses', ' How the waterpoint is managed', ' How the waterpoint is managed', ' What the water costs', ' What the water costs', ' The quality of the water', ' The quality of the water', ' The quantity of water', ' The quantity of water', ' The source of the water', ' The source of the water', ' The source of the water', ' The kind of waterpoint', ' The kind of waterpoint'))
kable(metadata,caption = 'Metadata')
```

## Appendix B

Source code for the project can be found here: https://github.com/StatisticsGuru/WaterPump 
